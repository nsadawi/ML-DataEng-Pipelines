{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis and Feature engineering for House Price modelling\n",
    "\n",
    "In this notebook we mix exploratory data analysis, to understand the nature of the data with feature engineering and feature selection. We make heavy use of the open source Python library [Feature-engine](https://feature-engine.readthedocs.io/en/latest/), because it contains several helpful functionalities.\n",
    "\n",
    "[Feature-engine](https://feature-engine.readthedocs.io/en/latest/)'s transformers come along with fit and transform funcionality, to learn the necessary parameters from the data, and then transform the data accordingly. So it is very easy to learn and use. And it can also detect automatically the nature of the variables, or engineer a selected group of variables.\n",
    "\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "**0. Feature Creation**\n",
    "\n",
    "    0.1 Binary indicators\n",
    "    \n",
    "    0.2 Variable Aggregations    \n",
    "    \n",
    "\n",
    "**1. Categorical variables**\n",
    "\n",
    "    1.1 Missing data\n",
    "\n",
    "    1.2 Cardinality\n",
    "    \n",
    "    1.3 Rare Labels\n",
    "    \n",
    "    1.4 Label grouping\n",
    "    \n",
    "    1.5 Variable selection\n",
    "    \n",
    "    1.6 Encoding\n",
    "    \n",
    "    \n",
    "**2. Time variables**\n",
    "\n",
    "    2.1 Time elapsed calculation\n",
    "    \n",
    "    2.2 Missing Data\n",
    "    \n",
    "    \n",
    "**3. Discrete variables**\n",
    "\n",
    "    3.1 Exploratory data analysis\n",
    "    \n",
    "    3.2 Missing Data\n",
    "    \n",
    "    \n",
    "**4. Numerical variables**\n",
    "\n",
    "    4.1 Distributions\n",
    "    \n",
    "    4.2 Relation with house sale price\n",
    "    \n",
    "    4.3 Missing Data\n",
    "    \n",
    "    \n",
    "**5. Machine learning modelling + cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's install Feature-engine\n",
    "#!pip install feature_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import feature_engine\n",
    "#feature_engine.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 1.331629,
     "end_time": "2021-01-05T18:54:08.546399",
     "exception": false,
     "start_time": "2021-01-05T18:54:07.21477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#from lightgbm import LGBMRegressor\n",
    "\n",
    "# Feature-engine's modules for feature egineering\n",
    "from feature_engine import creation\n",
    "from feature_engine import discretisation as disc\n",
    "from feature_engine import encoding as enc\n",
    "from feature_engine import imputation as imp\n",
    "from feature_engine import selection as sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.161779,
     "end_time": "2021-01-05T18:54:08.725737",
     "exception": false,
     "start_time": "2021-01-05T18:54:08.563958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load training data\n",
    "data = pd.read_csv('../Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1460 non-null   int64  \n",
      " 1   MSSubClass     1460 non-null   int64  \n",
      " 2   MSZoning       1460 non-null   object \n",
      " 3   LotFrontage    1201 non-null   float64\n",
      " 4   LotArea        1460 non-null   int64  \n",
      " 5   Street         1460 non-null   object \n",
      " 6   Alley          91 non-null     object \n",
      " 7   LotShape       1460 non-null   object \n",
      " 8   LandContour    1460 non-null   object \n",
      " 9   Utilities      1460 non-null   object \n",
      " 10  LotConfig      1460 non-null   object \n",
      " 11  LandSlope      1460 non-null   object \n",
      " 12  Neighborhood   1460 non-null   object \n",
      " 13  Condition1     1460 non-null   object \n",
      " 14  Condition2     1460 non-null   object \n",
      " 15  BldgType       1460 non-null   object \n",
      " 16  HouseStyle     1460 non-null   object \n",
      " 17  OverallQual    1460 non-null   int64  \n",
      " 18  OverallCond    1460 non-null   int64  \n",
      " 19  YearBuilt      1460 non-null   int64  \n",
      " 20  YearRemodAdd   1460 non-null   int64  \n",
      " 21  RoofStyle      1460 non-null   object \n",
      " 22  RoofMatl       1460 non-null   object \n",
      " 23  Exterior1st    1460 non-null   object \n",
      " 24  Exterior2nd    1460 non-null   object \n",
      " 25  MasVnrType     588 non-null    object \n",
      " 26  MasVnrArea     1452 non-null   float64\n",
      " 27  ExterQual      1460 non-null   object \n",
      " 28  ExterCond      1460 non-null   object \n",
      " 29  Foundation     1460 non-null   object \n",
      " 30  BsmtQual       1423 non-null   object \n",
      " 31  BsmtCond       1423 non-null   object \n",
      " 32  BsmtExposure   1422 non-null   object \n",
      " 33  BsmtFinType1   1423 non-null   object \n",
      " 34  BsmtFinSF1     1460 non-null   int64  \n",
      " 35  BsmtFinType2   1422 non-null   object \n",
      " 36  BsmtFinSF2     1460 non-null   int64  \n",
      " 37  BsmtUnfSF      1460 non-null   int64  \n",
      " 38  TotalBsmtSF    1460 non-null   int64  \n",
      " 39  Heating        1460 non-null   object \n",
      " 40  HeatingQC      1460 non-null   object \n",
      " 41  CentralAir     1460 non-null   object \n",
      " 42  Electrical     1459 non-null   object \n",
      " 43  1stFlrSF       1460 non-null   int64  \n",
      " 44  2ndFlrSF       1460 non-null   int64  \n",
      " 45  LowQualFinSF   1460 non-null   int64  \n",
      " 46  GrLivArea      1460 non-null   int64  \n",
      " 47  BsmtFullBath   1460 non-null   int64  \n",
      " 48  BsmtHalfBath   1460 non-null   int64  \n",
      " 49  FullBath       1460 non-null   int64  \n",
      " 50  HalfBath       1460 non-null   int64  \n",
      " 51  BedroomAbvGr   1460 non-null   int64  \n",
      " 52  KitchenAbvGr   1460 non-null   int64  \n",
      " 53  KitchenQual    1460 non-null   object \n",
      " 54  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 55  Functional     1460 non-null   object \n",
      " 56  Fireplaces     1460 non-null   int64  \n",
      " 57  FireplaceQu    770 non-null    object \n",
      " 58  GarageType     1379 non-null   object \n",
      " 59  GarageYrBlt    1379 non-null   float64\n",
      " 60  GarageFinish   1379 non-null   object \n",
      " 61  GarageCars     1460 non-null   int64  \n",
      " 62  GarageArea     1460 non-null   int64  \n",
      " 63  GarageQual     1379 non-null   object \n",
      " 64  GarageCond     1379 non-null   object \n",
      " 65  PavedDrive     1460 non-null   object \n",
      " 66  WoodDeckSF     1460 non-null   int64  \n",
      " 67  OpenPorchSF    1460 non-null   int64  \n",
      " 68  EnclosedPorch  1460 non-null   int64  \n",
      " 69  3SsnPorch      1460 non-null   int64  \n",
      " 70  ScreenPorch    1460 non-null   int64  \n",
      " 71  PoolArea       1460 non-null   int64  \n",
      " 72  PoolQC         7 non-null      object \n",
      " 73  Fence          281 non-null    object \n",
      " 74  MiscFeature    54 non-null     object \n",
      " 75  MiscVal        1460 non-null   int64  \n",
      " 76  MoSold         1460 non-null   int64  \n",
      " 77  YrSold         1460 non-null   int64  \n",
      " 78  SaleType       1460 non-null   object \n",
      " 79  SaleCondition  1460 non-null   object \n",
      " 80  SalePrice      1460 non-null   int64  \n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()\n",
    "tmp = data[['LotFrontage', 'LotArea','GarageArea', 'YearBuilt', 'YearRemodAdd','BsmtCond','Fence', 'RoofStyle', 'Heating', 'SalePrice']]\n",
    "tmp.to_csv('~/Downloads/house-price-data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(\n",
    "    ['Id', 'SalePrice'], axis=1),\n",
    "    data['SalePrice'],\n",
    "    test_size=0.05,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a dataset with all the train, test and subimssion, \n",
    "# so We can easily compare the variable distributions\n",
    "\n",
    "def create_master_data(train, test, y_train, y_test):\n",
    "    \n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "    \n",
    "    train['target'] = y_train\n",
    "    train['data'] = 'train'\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    test['target'] = y_test\n",
    "    test['data'] = 'test'\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    master_data = pd.concat([train, test], axis=0)\n",
    "    master_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return master_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target\n",
    "\n",
    "Let's explore the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogran to evaluate target distribution\n",
    "\n",
    "y_train.hist(bins=50, density=True)\n",
    "y_test.hist(bins=50, density=True)\n",
    "\n",
    "plt.legend([\"Train\", \"Test\"])\n",
    "plt.ylabel('Number of houses')\n",
    "plt.xlabel('Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the 2 distributions are very similar, because the datasets come from a random division of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's transform the target, we are optimising for the log \n",
    "# of the target as per competition rules\n",
    "\n",
    "np.log(y_train).hist(bins=50, density=True)\n",
    "np.log(y_test).hist(bins=50, density=True)\n",
    "\n",
    "plt.ylabel('Number of houses')\n",
    "plt.xlabel('Log of Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density plot instead of histogram\n",
    "np.log(y_train).plot.density()\n",
    "np.log(y_test).plot.density()\n",
    "\n",
    "plt.xlabel('Log of Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's transform the target with the log\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "\n",
    "y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation\n",
    "\n",
    "In this section, We create new variables, from those already in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the house has a pool\n",
    "X_train['HasPool'] = np.where(X_train['PoolArea']>0,1,0)\n",
    "X_test['HasPool'] = np.where(X_test['PoolArea']>0,1,0)\n",
    "\n",
    "\n",
    "# if the house has an Alley\n",
    "X_train['HasAlley'] = np.where(X_train['Alley'].isnull(), 0, 1)\n",
    "X_test['HasAlley'] = np.where(X_test['Alley'].isnull(), 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to plot the median sale price vs a list of variables\n",
    "\n",
    "def plot_median_price(variables, limits):\n",
    "    \n",
    "    tmp = pd.concat([X_train[variables].reset_index(drop=True),\n",
    "                 y_train.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # plot mean sale price per null value or otherwise\n",
    "    g = sns.PairGrid(tmp, x_vars=variables, y_vars=['SalePrice'])\n",
    "    g.map(sns.barplot)\n",
    "    plt.ylim(limits)   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean house sale price for the created variables\n",
    "# to see if they add some value\n",
    "\n",
    "plot_median_price(['HasAlley', 'HasPool'], (1.5,19.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not clear if these variables add a lot of value. The difference in price is small, but then the price is in log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variables Condition1 and Condition2 indicate Proximity to various conditions. \n",
    "\n",
    "# Create a variable that aggregates both to indicate if the house is\n",
    "# close to more than 1 ammenity. If the house is close to more than 1\n",
    "# ammenity, the conditions take different values.\n",
    "\n",
    "X_train['Condition_total'] = np.where(X_train['Condition2'] == X_train['Condition1'], 0, 1)\n",
    "X_test['Condition_total'] = np.where(X_test['Condition2'] == X_test['Condition1'], 0, 1)\n",
    "\n",
    "# inspect the variable\n",
    "X_train['Condition_total'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly 13% of the houses are proximal to 2 different ammenities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exterior 1s and 2nd indicate the Exterior material covering on house:\n",
    "# Create a variable that aggregates both, like we did for contition\n",
    "\n",
    "X_train['Exterior_total'] = np.where(X_train['Exterior1st'] == X_train['Exterior2nd'], 0, 1)\n",
    "X_test['Exterior_total'] = np.where(X_test['Exterior1st'] == X_test['Exterior2nd'], 0, 1)\n",
    "\n",
    "# inspect the variable\n",
    "X_train['Exterior_total'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly 15% of the houses ashow 2 materials in the exterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean house sale price for the created variables\n",
    "\n",
    "plot_median_price(['Condition_total', 'Exterior_total'], (1.5,16.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it is unclear if these new variables add a lot of value. The difference in price seems minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's identify the categorical variables\n",
    "\n",
    "categorical = [var for var in data.columns if data[var].dtype == 'O']\n",
    "\n",
    "# MSSubClass is also categorical by definition, despite its numeric values\n",
    "categorical = categorical + ['MSSubClass']\n",
    "\n",
    "# number of categorical variables\n",
    "len(categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast all variables as categorical\n",
    "\n",
    "X_train[categorical] = X_train[categorical].astype('O')\n",
    "X_test[categorical] = X_test[categorical].astype('O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which categorical variables have missing data?\n",
    "\n",
    "# capture categorical variables with NA in a dictionary\n",
    "null_cat = {var: data[var].isnull().mean() for var in categorical if data[var].isnull().mean()>0}\n",
    "\n",
    "# plot\n",
    "pd.Series(null_cat).sort_values().plot.bar(figsize=(10,4))\n",
    "plt.ylabel('Percentage of missing data')\n",
    "plt.axhline(y = 0.90, color = 'r', linestyle = '-') \n",
    "plt.axhline(y = 0.80, color = 'g', linestyle = '-') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few variables with a lot of data missing, and a few variables with few missing data. In particular, there are 3 variables for which > 90% of the values are missing (above red line)\n",
    "\n",
    "Let's examine if the variables with NA are somewhat informative of the house sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temporary dataset with the interest variables\n",
    "\n",
    "tmp = pd.concat([X_train[['Alley', 'MiscFeature', 'PoolQC']].reset_index(drop=True),\n",
    "                 y_train.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# replace null values by 1, or 0 otherwise\n",
    "for var in ['Alley', 'MiscFeature', 'PoolQC']:\n",
    "    tmp[var] = np.where(tmp[var].isnull(),1,0)\n",
    "\n",
    "\n",
    "# plot mean sale price per null value or otherwise\n",
    "g = sns.PairGrid(tmp, x_vars=['Alley', 'MiscFeature', 'PoolQC'], y_vars=['SalePrice'])\n",
    "g.map(sns.barplot)\n",
    "plt.ylim(1,15)   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They don't seem to be hughly predictive so we will drop them (we already captured 2 of them in a binary feature anyways, at the beginning of the notebook).\n",
    "\n",
    "[DropFeatures](https://feature-engine.readthedocs.io/en/latest/selection/DropFeatures.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DropFeatures allows you to drop selected feature groups from data\n",
    "\n",
    "drop_features = sel.DropFeatures(features_to_drop = ['Alley', 'MiscFeature', 'PoolQC'])\n",
    "\n",
    "X_train = drop_features.fit_transform(X_train)\n",
    "X_test = drop_features.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's impute the missing data in categorical variables.\n",
    "\n",
    "[CategoricalImputer](https://feature-engine.trainindata.com/en/latest/api_doc/imputation/CategoricalImputer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing data, categorical variables are detected automatically\n",
    "# the imputer replaces missing data with the string 'Missing'\n",
    "\n",
    "cat_imputer = imp.CategoricalImputer(return_object=True)\n",
    "\n",
    "cat_imputer.fit(X_train)\n",
    "\n",
    "# the variables to impute are stored in the variables attribute\n",
    "cat_imputer.variables_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of categorical variables detected by Feature-engine\n",
    "\n",
    "len(cat_imputer.variables_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the imputer will add a string 'Missing' to each categorical variable\n",
    "\n",
    "cat_imputer.imputer_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove missing data\n",
    "\n",
    "X_train = cat_imputer.transform(X_train)\n",
    "X_test = cat_imputer.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we do not have more missing data in categorical variables\n",
    "# if we do, the list should not be empty\n",
    "\n",
    "[c for c in cat_imputer.feature_names_in_ if X_test[c].isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasi-constant variables\n",
    "\n",
    "Let's inspect if there are some variables that show predominantly 1 value  in all observations. We can find them automatically with the folliwing class from Feature-engine:\n",
    "\n",
    "[DropConstantFeatures](https://feature-engine.readthedocs.io/en/latest/selection/DropConstantFeatures.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the categorical variable list to the categorical variable name\n",
    "\n",
    "categorical = cat_imputer.variables_\n",
    "\n",
    "# we still have 41 categorical variables in the dataset\n",
    "len(categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We ask the transformer to remove all variables that show the same value in more than\n",
    "# 94% of the observations (tol=0.94)\n",
    "constant = sel.DropConstantFeatures(tol=0.94, variables=categorical)\n",
    "\n",
    "# find constant features\n",
    "constant.fit(X_train)\n",
    "\n",
    "# the quasi-constant features are stored in this attribute\n",
    "constant.features_to_drop_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put data together for analysis\n",
    "\n",
    "data = create_master_data(X_train, X_test, y_train, y_test)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of observations per category, per variable\n",
    "# so that we see that these variables show mostly 1 value in most of the \n",
    "# observations\n",
    "\n",
    "for variable in constant.features_to_drop_:\n",
    "    \n",
    "    data.groupby(variable)['data'].value_counts().unstack().plot.bar(figsize=(10,5))\n",
    "    plt.title(variable)\n",
    "    plt.ylabel('Number of houses')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in most of these variables, most observations show the same value. We will re-map these variables to group all the categories that are less frequent into a new category called 'Other'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will re-group these variables into either the majoritarian category\n",
    "# or \"Other\"\n",
    "\n",
    "# if a category is present in less than 10% of the observations, we group it with other\n",
    "# infrequent categories (tol param)\n",
    "\n",
    "rare_enc = enc.RareLabelEncoder(tol = 0.1,\n",
    "                                n_categories=1, # number of minimum categories per variable for the grouping to proceed\n",
    "                                variables=constant.features_to_drop_, # the variables to pre-process\n",
    "                                replace_with='Other', # the label to use to replace the original category\n",
    "                               )\n",
    "\n",
    "rare_enc.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the grouping\n",
    "\n",
    "X_train = rare_enc.transform(X_train)\n",
    "X_test = rare_enc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, when we plot the data again, we should see only 2 categories in each variable\n",
    "\n",
    "# put data together for analysis\n",
    "data = create_master_data(X_train, X_test, y_train, y_test)\n",
    "\n",
    "for variable in constant.features_to_drop_:\n",
    "    \n",
    "    data.groupby(variable)['data'].value_counts().unstack().plot.bar(figsize=(10,5))\n",
    "    plt.title(variable)\n",
    "    plt.ylabel('Number of houses')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality variables\n",
    "\n",
    "There are a number of variables that refer to the quality of some aspect of the house, for example the garage, or the fence, or the kitchen. I will replace these categories by numbers increasing with the quality of the place or room.\n",
    "\n",
    "- Ex = Excellent\n",
    "- Gd = Good\n",
    "- TA = Average/Typical\n",
    "- Fa =\tFair\n",
    "- Po = Poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_mappings = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, 'Missing': 0, 'NA': 0}\n",
    "\n",
    "qual_vars = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',\n",
    "             'HeatingQC', 'KitchenQual', 'FireplaceQu',\n",
    "             'GarageQual', 'GarageCond',\n",
    "            ]\n",
    "\n",
    "for var in qual_vars:\n",
    "    X_train[var] = X_train[var].map(qual_mappings)\n",
    "    X_test[var] = X_test[var].map(qual_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure_mappings = {'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4, 'Missing': 0, 'NA': 0}\n",
    "\n",
    "var = 'BsmtExposure'\n",
    "\n",
    "X_train[var] = X_train[var].map(exposure_mappings)\n",
    "X_test[var] = X_test[var].map(exposure_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finish_mappings = {'Missing': 0, 'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}\n",
    "\n",
    "finish_vars = ['BsmtFinType1', 'BsmtFinType2']\n",
    "\n",
    "for var in finish_vars:\n",
    "    X_train[var] = X_train[var].map(finish_mappings)\n",
    "    X_test[var] = X_test[var].map(finish_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garage_mappings = {'Missing': 0, 'NA': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}\n",
    "\n",
    "var = 'GarageFinish'\n",
    "\n",
    "X_train[var] = X_train[var].map(garage_mappings)\n",
    "X_test[var] = X_test[var].map(garage_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fence_mappings = {'Missing': 0, 'NA': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4}\n",
    "\n",
    "var = 'Fence'\n",
    "\n",
    "X_train[var] = X_train[var].map(fence_mappings)\n",
    "X_test[var] = X_test[var].map(fence_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture all quality variables\n",
    "\n",
    "qual_vars  = qual_vars + finish_vars + ['BsmtExposure','GarageFinish','Fence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's plot the house mean sale price based on the quality of the \n",
    "# various attributes\n",
    "\n",
    "# put data together for analysis\n",
    "data = create_master_data(X_train, X_test, y_train, y_test)\n",
    "\n",
    "for var in qual_vars:\n",
    "    # make boxplot with Catplot\n",
    "    sns.catplot(x=var, y='target', data=data[data['data']=='train'], kind=\"box\", height=4, aspect=1.5)\n",
    "    # add data points to boxplot with stripplot\n",
    "    sns.stripplot(x=var, y='target', data=data[data['data']=='train'], jitter=0.1, alpha=0.3, color='k')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many quality variables, we see an increase in the house price, with the quality.\n",
    "\n",
    "### Cardinality\n",
    "\n",
    "Cardinality indicates the number of unique values or categories per variable. The highest the cardinality, the more difficult the variable is to handle. Although it could provide more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's collect the remaining categorical variables\n",
    "\n",
    "categorical = [c for c in categorical if c not in constant.features_to_drop_]\n",
    "\n",
    "categorical = [c for c in categorical if c not in qual_vars]\n",
    "\n",
    "len(categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with nunique from pandas we count the number of unique categories per variable\n",
    "\n",
    "data[categorical].nunique().sort_values().plot.bar(figsize=(15,5))\n",
    "plt.ylabel('Number of categories per variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some variables have few categories (low cardinality), others have a lot of categories (high cardinality). \n",
    "\n",
    "In addition to the number of unique categories, it is important to know how many observations in the dataset show each category. Rare categories tend to bring problems when building ML models, with little reliable information, precisely, because there are not a lot of observations to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of observations / houses per category, per variable\n",
    "\n",
    "for variable in categorical:\n",
    "    data.groupby(variable)['data'].value_counts().unstack().plot.bar(figsize=(10,5))\n",
    "    plt.title(variable)\n",
    "    plt.ylabel('Number of houses')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that pretty much for every variable, some categories are shared by many houses, and some are shown only by very few houses. Then, we wouldn't know if we should trust the Sale Price of houses with these few categories, because we have few houses to learn from.\n",
    "\n",
    "Another problem that comes up with rare categories, is that they may appear only in the train set or only in the test set. So if it appears only in the train set, they may cause over-fitting. But if it appears only on the test set, then the model does not know what to make of that category, and it may crash.\n",
    "\n",
    "**We will group infrequent labels together into one umbrella category called 'Rare'**. But before that, let's have a look at the mean sale price per category, to see if we find some value in any of these variables that we can capitalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function calculates:\n",
    "\n",
    "# 1) the percentage of houses per category\n",
    "# 2) the mean SalePrice per category\n",
    "\n",
    "# and returns a dataframe with those 2 variables\n",
    "\n",
    "def calculate_mean_target_per_category(df, var):\n",
    "    \n",
    "    df = pd.concat([df, y_train], axis=1)\n",
    "    \n",
    "    # total number of houses\n",
    "    total_houses = len(df)\n",
    "\n",
    "    # percentage of houses per category\n",
    "    temp_df = pd.Series(df[var].value_counts() / total_houses).reset_index()\n",
    "    temp_df.columns = [var, 'perc_houses']\n",
    "\n",
    "    # add the mean SalePrice\n",
    "    temp_df = temp_df.merge(df.groupby([var])['SalePrice'].mean().reset_index(),\n",
    "                            on=var,\n",
    "                            how='left')\n",
    "\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we use the function for the variable 'Neighborhood'\n",
    "temp_df = calculate_mean_target_per_category(X_train, 'Neighborhood')\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a function to plot the\n",
    "# category frequency and mean SalePrice.\n",
    "\n",
    "# This will help us visualise the relationship between the\n",
    "# target and the labels of the categorical variable\n",
    "\n",
    "def plot_categories(df, var):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    plt.xticks(df.index, df[var], rotation=90)\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax.bar(df.index, df[\"perc_houses\"], color='lightgrey')\n",
    "    ax2.plot(df.index, df[\"SalePrice\"], color='green', label='Seconds')\n",
    "    ax.axhline(y=0.05, color='red')\n",
    "    ax.set_ylabel('percentage of houses per category')\n",
    "    ax.set_xlabel(var)\n",
    "    ax2.set_ylabel('Average Sale Price per category')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot house count and sale price for Neighbourhood.\n",
    "\n",
    "plot_categories(temp_df, 'Neighborhood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we can see that there are expensive neighbourhoods and cheap neighbourhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of the least expensive neighbourhoods\n",
    "\n",
    "cheap_neighbourhoods = temp_df[temp_df['SalePrice']<2.47]['Neighborhood'].unique()\n",
    "\n",
    "cheap_neighbourhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of the most expensive neighbourhoods\n",
    "\n",
    "expensive_neighbourhoods = temp_df[temp_df['SalePrice']>=2.47]['Neighborhood'].unique()\n",
    "\n",
    "expensive_neighbourhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new variable that segregates neigbourhoods as per the previous lists\n",
    "\n",
    "X_train['Neigh_Price'] = np.where(X_train['Neighborhood'].isin(cheap_neighbourhoods),\n",
    "                                  0, np.where(X_train['Neighborhood'].isin(expensive_neighbourhoods),\n",
    "                                              2, 1))\n",
    "\n",
    "X_test['Neigh_Price'] = np.where(X_test['Neighborhood'].isin(cheap_neighbourhoods),\n",
    "                                  0, np.where(X_test['Neighborhood'].isin(expensive_neighbourhoods),\n",
    "                                              2, 1))\n",
    "\n",
    "# let's inspect the new variable\n",
    "X_train['Neigh_Price'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's inspect the value of our new variable\n",
    "\n",
    "# put data together for analysis\n",
    "data = create_master_data(X_train, X_test, y_train, y_test)\n",
    "\n",
    "sns.catplot(x='Neigh_Price', y='target', data=data[data['data']=='train'], kind=\"box\", height=4, aspect=1.5)\n",
    "# add data points to boxplot with stripplot\n",
    "sns.stripplot(x='Neigh_Price', y='target', data=data[data['data']=='train'], jitter=0.1, alpha=0.3, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot for the remaining categorical variables\n",
    "# the count of houses per category and the mean sale price\n",
    "\n",
    "for col in categorical:\n",
    "    \n",
    "    # we plotted this variable already\n",
    "    if col !='Neighborhood':\n",
    "        \n",
    "        # re using the functions I created\n",
    "        temp_df = calculate_mean_target_per_category(data, col)\n",
    "        plot_categories(temp_df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other variables, don't show a clear increase in house for certain categories. So we will not derive new features from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will group infrequent labels together:\n",
    "\n",
    "# if a category is present in less than 5% of the observations, we group it with other\n",
    "# infrequent categories (tol param)\n",
    "\n",
    "rare_enc = enc.RareLabelEncoder(tol = 0.05,\n",
    "                                n_categories=4, # number of minimum categories per variable for the grouping to proceed\n",
    "                                variables=categorical\n",
    "                               )\n",
    "\n",
    "rare_enc.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warnings tell us that there are a few categorical variables that we indicated to the variables parameter, that have less than 4 unique categories. So the transformer will not pre-process those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = rare_enc.transform(X_train)\n",
    "X_test = rare_enc.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put data together for analysis\n",
    "\n",
    "data = create_master_data(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can plot the variables with grouped categories\n",
    "\n",
    "for variable in categorical:\n",
    "    data.groupby(variable)['data'].value_counts().unstack().plot.bar(figsize=(10,5))\n",
    "    plt.title(variable)\n",
    "    plt.ylabel('Number of houses')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that after grouping the variables, the categories appear in all the two datasets, train and test. And also, they are shared by at least more than 5% of the observations in the datasets.\n",
    "\n",
    "### Categorical variable importance\n",
    "\n",
    "Let's plot the House Sale Price distribution per category per variable to understand if there is a difference.\n",
    "\n",
    "**Boxplot**: indicates the median value of the house, and the interquartile range distance, which contains most of the houses. The rombos above and below are outliers for that distribution.\n",
    "\n",
    "**Jitter**: on top of the box plot we plot the houses individually as dots, this gives us an idea of how many houses show that category. More dots, more houses.\n",
    "\n",
    "If the box plots are at the same height, then the categories or the variable do not show predictive power. But if the show different heights for the different categories, then they might."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in categorical:\n",
    "    # make boxplot with Catplot\n",
    "    sns.catplot(x=variable, y='target', data=data[data['data']=='train'], kind=\"box\", height=4, aspect=1.5)\n",
    "    # add data points to boxplot with stripplot\n",
    "    sns.stripplot(x=variable, y='target', data=data[data['data']=='train'], jitter=0.1, alpha=0.3, color='k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some categories seem to correlate with higher or lower house sale prices. And some, not at all. See for example the plot for the variable **Foundation**. The different foundations seem to correlate with lower sale prices. The same is true for the variable **SaleCondition** and others.\n",
    "\n",
    "We can try and determine which of the variables are useful using an approach used in the KDD2009 data science competition. The approach consists in replacing the category by the mean of the target. And then using that replacement as a prediction, and evaluate the performance comparing the prediction to the real value of the house price.\n",
    "\n",
    "We can perform all of this, very easily with a class from Feature-engine.\n",
    "\n",
    "Check [SelectByTargetMeanPerformance](https://feature-engine.readthedocs.io/en/latest/selection/SelectByTargetMeanPerformance.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical variables\n",
    "\n",
    "To use categorical variables in machine learning models, we need to encode them into numbers. At least for the Gradient Boosting Classifier from Sklearn.\n",
    "\n",
    "We are going to assign numbers to the variables, but these numbers will be assigned from smaller to bigger, based on the mean sale price per category. This way, we create (hopefully) a monotonic relationship between the encoded variable and the target, which may boost the performance of the model.\n",
    "\n",
    "[OrdinalEncoder](https://feature-engine.readthedocs.io/en/latest/encoding/OrdinalEncoder.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's identify all remaining categorical variables (remember that we encoded\n",
    "# some into numbers already)\n",
    "\n",
    "categorical = [var for var in X_train.columns if X_train[var].dtype == 'O']\n",
    "\n",
    "# MSSubClass is also categorical by definition, despite its numeric values\n",
    "#categorical = categorical + ['MSSubClass']\n",
    "\n",
    "# number of categorical variables\n",
    "len(categorical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast all variables as categorical\n",
    "\n",
    "#X_train[categorical] = X_train[categorical].astype('O')\n",
    "#X_test[categorical] = X_test[categorical].astype('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the ordinal encoder from Feature-engine\n",
    "\n",
    "encoder = enc.OrdinalEncoder(encoding_method='ordered',\n",
    "                             variables=categorical)\n",
    "\n",
    "encoder.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this attribute we find the numbers that will replace each category in each variable\n",
    "\n",
    "encoder.encoder_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the variables\n",
    "\n",
    "X_train = encoder.transform(X_train)\n",
    "X_test = encoder.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's examine the monotonic relationships\n",
    "\n",
    "for var in categorical:\n",
    "    plt.scatter(X_train[var], y_train, alpha=0.2)\n",
    "    plt.ylabel('Log of Sale Price')\n",
    "    plt.xlabel(var)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some variables, we see an increase in the house price with the encoded variable value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that NAN were not introduced during the encoding\n",
    "\n",
    "[c for c in categorical if X_train[c].isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that NAN were not introduced during the encoding\n",
    "\n",
    "[c for c in categorical if X_test[c].isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time variables\n",
    "\n",
    "There are a few variables that show time, for example the year in which the house was sold, or the garage was built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_vars = [var for var in data.columns if 'Yr' in var or 'Year' in var]\n",
    "\n",
    "year_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the sale price over the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_master_data(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make boxplot with Catplot\n",
    "sns.catplot(x='YrSold', y='target', data=data[data['data']=='train'], kind=\"box\", height=4, aspect=1.5)\n",
    "# add data points to boxplot with stripplot\n",
    "sns.stripplot(x='YrSold', y='target', data=data[data['data']=='train'], jitter=0.1, alpha=0.3, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the sale price seems stable over the years for which we have data.\n",
    "\n",
    "Now let's plot the house price vs the last time where something was remodelled in the house. I expect more expensive houses if they were recently remodelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n",
    "    plt.scatter(X_train[variable], y_train)\n",
    "    plt.ylabel('Log of sale Price')\n",
    "    plt.xlabel(variable)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a slight linear trend to increase in house prices, for those houses that were remodelled in later years.\n",
    "\n",
    "Let's capture the difference between the year in which something was remodelled or built and the sale time.\n",
    "\n",
    "[RelativeFeatures](https://feature-engine.trainindata.com/en/latest/api_doc/creation/RelativeFeatures.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this transformer will substract all the variables in the reference list, from YrSold\n",
    "# to determine the age of remodelling at point of sale\n",
    "\n",
    "create = creation.RelativeFeatures(\n",
    "    variables = ['YrSold'],\n",
    "    reference = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'],\n",
    "    func=['sub']\n",
    ")\n",
    "\n",
    "create.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = create.transform(X_train)\n",
    "X_test = create.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the new variables at the right of the dataframe\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the variable 'YearRemodAdd' shows the same value as the\n",
    "# variable 'YearBuilt', that means that the house has not been remodelled\n",
    "\n",
    "# so let's create a new feature that captures this.\n",
    "\n",
    "remodelled = creation.RelativeFeatures(\n",
    "    variables = ['YearRemodAdd'],\n",
    "    reference = ['YearBuilt'],\n",
    "    func=['sub'],\n",
    ")\n",
    "\n",
    "remodelled.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = remodelled.transform(X_train)\n",
    "X_test = remodelled.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the new variable at the right of the dataframe\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to remove the original time features\n",
    "\n",
    "drop_features = sel.DropFeatures(features_to_drop =['YearBuilt', 'YearRemodAdd', 'GarageYrBlt','YrSold'])\n",
    "\n",
    "X_train = drop_features.fit_transform(X_train)\n",
    "X_test = drop_features.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if the variables have missing data\n",
    "\n",
    "X_train[[\n",
    "    'YrSold_sub_YearBuilt', 'YrSold_sub_YearRemodAdd' ,'YrSold_sub_GarageYrBlt', 'YearRemodAdd_sub_YearBuilt']\n",
    "].isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[[\n",
    "    'YrSold_sub_YearBuilt', 'YrSold_sub_YearRemodAdd' ,'YrSold_sub_GarageYrBlt', 'YearRemodAdd_sub_YearBuilt']\n",
    "].isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YrSold_sub_GarageYrBlt shows missing data. We will impute this altogether when we impute numerical variables later on in the notebook.\n",
    "\n",
    "The final temporal variable is the month in which the house was sold. Let's check if there is some price change depending on the month of the sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make boxplot with Catplot\n",
    "sns.catplot(x='MoSold', y='target', data=data[data['data']=='train'], kind=\"box\", height=4, aspect=1.5)\n",
    "# add data points to boxplot with stripplot\n",
    "sns.stripplot(x='MoSold', y='target', data=data[data['data']=='train'], jitter=0.1, alpha=0.3, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean house sale price seems similar throughout the months, but there is a difference in the number of houses sold, with less houses sold in Jan and Feb, and more houses sold towards the summer months of the northern hemisphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete = [\n",
    "    var for var in X_train.columns if X_train[var].dtype != 'O'\n",
    "    and var not in year_vars\n",
    "    and var not in qual_vars+categorical\n",
    "    and len(X_train[var].unique()) < 20 \n",
    "]\n",
    "\n",
    "# number of discrete variables\n",
    "len(discrete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there is a relationship between the values of discrete houses and the mean sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in discrete:\n",
    "    # make boxplot with Catplot\n",
    "    sns.catplot(x=var, y='target', data=data[data['data']=='train'], kind=\"box\", height=4, aspect=1.5)\n",
    "    # add data points to boxplot with stripplot\n",
    "    sns.stripplot(x=var, y='target', data=data[data['data']=='train'], jitter=0.1, alpha=0.3, color='k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most of these variables, there is a correlation between the number assigned to the house and the sale price. Particularly for those variables which values are determined by people, for example OverallQual, or OverallCond. But also for variables like number of bathrooms or rooms.\n",
    "\n",
    "The only variable that seems to have only 1 value predominantly is PoolArea, so we will remove it from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove original feature\n",
    "\n",
    "drop_features = sel.DropFeatures(features_to_drop =['PoolArea'])\n",
    "\n",
    "X_train = drop_features.fit_transform(X_train)\n",
    "X_test = drop_features.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Missing data\n",
    "\n",
    "# let's capture the discrete variables with missing data\n",
    "\n",
    "null_disc = {var: data[var].isnull().mean() for var in discrete if data[var].isnull().mean()>0}\n",
    "\n",
    "# plot\n",
    "if len(null_disc) > 0:\n",
    "    pd.Series(null_disc).sort_values().plot.bar(figsize=(10,4))\n",
    "    plt.ylabel('Percentage of missing data')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there discrete variables with NA, we will impute these altogether with numerical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get variable names\n",
    "\n",
    "pd.Series(null_disc).sort_values().index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.038643,
     "end_time": "2021-01-05T18:54:08.8858",
     "exception": false,
     "start_time": "2021-01-05T18:54:08.847157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "numerical = [\n",
    "    var for var in X_train.columns \n",
    "    if var not in categorical + qual_vars + discrete + year_vars\n",
    "]\n",
    "\n",
    "len(numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's examine the distribution of the numerical continuous variables\n",
    "\n",
    "X_train[numerical].hist(bins=50, figsize=(15,15))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some variables that show predominantly 1 value, like MiscVal, ScreenPorch, LowQualFinSF. We could remove them with the constant features selector. But we could also let the model decide if they are important or not. So we will do this, this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the sale price vs the numerical variables\n",
    "\n",
    "tmp = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "sns.pairplot(data=tmp,\n",
    "             y_vars='SalePrice',\n",
    "             x_vars=['LotFrontage',\n",
    "                     'LotArea',\n",
    "                     'MasVnrArea',\n",
    "                     'BsmtFinSF1',\n",
    "                     'BsmtFinSF2' ]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the higher the value of the variable, the higher the sale price, for most of these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=tmp,\n",
    "             y_vars=['SalePrice'],\n",
    "             x_vars=['BsmtUnfSF',\n",
    "                     'TotalBsmtSF',\n",
    "                     '1stFlrSF',\n",
    "                     '2ndFlrSF',\n",
    "                     'LowQualFinSF' ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same here, the higher the value of the variable, the higher the sale price, in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=tmp,\n",
    "             y_vars=['SalePrice'],\n",
    "             x_vars=['GrLivArea',\n",
    "                     'GarageArea',\n",
    "                     'WoodDeckSF',\n",
    "                     'OpenPorchSF',\n",
    "                     'EnclosedPorch' ])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=tmp,\n",
    "             y_vars=['SalePrice'],\n",
    "             x_vars=['3SsnPorch',\n",
    "                     'ScreenPorch',\n",
    "                     'MiscVal'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these variables, there does not seem to be a clear tendency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now the time variables we created before\n",
    "\n",
    "sns.pairplot(data=tmp,\n",
    "             y_vars=['SalePrice'],\n",
    "             x_vars=['YrSold_sub_YearBuilt', \n",
    "                     'YrSold_sub_YearRemodAdd' ,\n",
    "                     'YrSold_sub_GarageYrBlt',\n",
    "                     'YearRemodAdd_sub_YearBuilt'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some promise in these variables\n",
    "\n",
    "### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.042698,
     "end_time": "2021-01-05T18:54:08.947435",
     "exception": false,
     "start_time": "2021-01-05T18:54:08.904737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concatenate data sources\n",
    "data = create_master_data(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# capture numerical variables with NA\n",
    "null_num = {var: data[var].isnull().mean() for var in numerical if data[var].isnull().mean()>0}\n",
    "\n",
    "# plot\n",
    "pd.Series(null_num).sort_values().plot.bar(figsize=(5,4))\n",
    "plt.ylabel('Percentage of missing data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few variables that show missing data.\n",
    "\n",
    "Let's add a missing indicator first and then impute the missing data with the median value of the variable. \n",
    "\n",
    "[MeanMedianImputer](https://feature-engine.readthedocs.io/en/latest/imputation/MeanMedianImputer.html)\n",
    "\n",
    "[AddMissingIndicator](https://feature-engine.readthedocs.io/en/1.0.x/imputation/AddMissingIndicator.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_impute = [c for c in null_num.keys()]\n",
    "\n",
    "# add the discrete variables with NA\n",
    "vars_to_impute = vars_to_impute + ['GarageCars', 'BsmtFullBath', 'BsmtHalfBath']\n",
    "\n",
    "indicator = imp.AddMissingIndicator(missing_only = False,\n",
    "                                variables = vars_to_impute,\n",
    "                               )\n",
    "\n",
    "indicator.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this attribute we find the variables for which\n",
    "# the missing indicators will be added\n",
    "\n",
    "indicator.variables_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = indicator.transform(X_train)\n",
    "X_test = indicator.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.054805,
     "end_time": "2021-01-05T18:54:09.235925",
     "exception": false,
     "start_time": "2021-01-05T18:54:09.18112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now replace NA by the median value\n",
    "\n",
    "num_imputer = imp.MeanMedianImputer(imputation_method = 'median',\n",
    "                                    variables = vars_to_impute,\n",
    "                                   )\n",
    "\n",
    "num_imputer.fit(X_train)\n",
    "\n",
    "# the median for imputation for each numerical variable\n",
    "num_imputer.imputer_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.035819,
     "end_time": "2021-01-05T18:54:09.293677",
     "exception": false,
     "start_time": "2021-01-05T18:54:09.257858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = num_imputer.transform(X_train)\n",
    "X_test = num_imputer.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in X_train.columns if X_train[c].isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in X_test.columns if X_test[c].isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some more features\n",
    "\n",
    "# total number of basement bathrooms\n",
    "bath_bsmt = creation.MathFeatures(\n",
    "    variables=['BsmtHalfBath', 'BsmtFullBath'],\n",
    "    func=['sum'],\n",
    "    new_variables_names=['BsmtBath_total'],\n",
    ")\n",
    "\n",
    "# total number of bathrooms\n",
    "bath_ground = creation.MathFeatures(\n",
    "    variables=['FullBath', 'HalfBath'],\n",
    "    func=['sum'],\n",
    "    new_variables_names=['Bath_total'],\n",
    ")\n",
    "\n",
    "X_train = bath_bsmt.fit_transform(X_train)\n",
    "X_test = bath_bsmt.transform(X_test)\n",
    "\n",
    "X_train = bath_ground.fit_transform(X_train)\n",
    "X_test = bath_ground.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with nested cross-validation\n",
    "Nice explanation here:\n",
    "https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cross_val(model, grid):\n",
    "    \n",
    "    # configure the cross-validation procedure\n",
    "    cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    # enumerate splits\n",
    "    outer_results = list()\n",
    "\n",
    "    for train_ix, test_ix in cv_outer.split(X_train):\n",
    "\n",
    "        # split data\n",
    "        xtrain, xtest = X_train.loc[train_ix, :], X_train.loc[test_ix, :]\n",
    "        ytrain, ytest = y_train[train_ix], y_train[test_ix]\n",
    "\n",
    "        # configure the cross-validation procedure\n",
    "        cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "        # define search\n",
    "        search = GridSearchCV(model, grid, scoring='neg_root_mean_squared_error', cv=cv_inner, refit=True)\n",
    "\n",
    "        # execute search\n",
    "        search.fit(xtrain, ytrain)\n",
    "\n",
    "        # evaluate model on the hold out dataset\n",
    "        yhat = search.predict(xtest)\n",
    "\n",
    "        # evaluate the model\n",
    "        rmse = mean_squared_error(ytest, yhat, squared=False)\n",
    "\n",
    "        # store the result\n",
    "        outer_results.append(rmse)\n",
    "\n",
    "        # report progress\n",
    "        print('>rmse_outer=%.3f, rmse_inner=%.3f, cfg=%s' % (rmse, search.best_score_, search.best_params_))\n",
    "\n",
    "    # summarize the estimated performance of the model\n",
    "    print('rmse_outer: %.3f +- %.3f' % (np.mean(outer_results), np.std(outer_results)))\n",
    "    \n",
    "    return search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM\n",
    "Takes a long time!\n",
    "Six minutes on my laptop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbm = GradientBoostingRegressor(\n",
    "    loss='ls',\n",
    "    n_estimators=100,\n",
    "    criterion='friedman_mse',\n",
    "    min_samples_split=2,\n",
    "    max_depth=3,\n",
    "    random_state=0,\n",
    "    n_iter_no_change=2,\n",
    "    tol=0.0001,\n",
    "    )\n",
    "\n",
    "param_grid = dict(\n",
    "    loss=['ls', 'huber'],\n",
    "    n_estimators=[10, 20, 50, 100, 200, 500, 1000, 2000],\n",
    "    min_samples_split=[0.1, 0.3, 0.5],\n",
    "    max_depth=[1,2,3,4,None],\n",
    "    )\n",
    "\n",
    "search = nested_cross_val(gbm, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(search.cv_results_)\n",
    "\n",
    "results.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
    "\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results[['params','mean_test_score', 'std_test_score']]\n",
    "\n",
    "results['mean_test_score'].plot(yerr=[results['std_test_score'], results['std_test_score']], subplots=True)\n",
    "\n",
    "plt.ylabel('Mean test score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the predictions\n",
    "\n",
    "X_train_preds = search.predict(X_train)\n",
    "X_test_preds = search.predict(X_test)\n",
    "\n",
    "\n",
    "print('Train rmse: ', mean_squared_error(y_train, X_train_preds, squared=False))\n",
    "print('Test rmse: ', mean_squared_error(y_test, X_test_preds, squared=False))\n",
    "print()\n",
    "print('Train r2: ', r2_score(y_train, X_train_preds))\n",
    "print('Test r2: ', r2_score(y_test, X_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember house price has been logged so we should exponentiate!\n",
    "# np.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM\n",
    "Takes really long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Light GBM\n",
    "\n",
    "lgbm_param = {\n",
    "    \"num_leaves\": [6, 8, 20, 30],\n",
    "    \"max_depth\": [2, 4, 6, 8, 10],\n",
    "    \"n_estimators\": [50, 100, 200, 500],\n",
    "    'colsample_bytree': [0.3, 1.0],\n",
    "}\n",
    "\n",
    "lgbm = LGBMRegressor(\n",
    "    learning_rate = 0.1, \n",
    "    min_child_weight = 0.4,\n",
    "    objective='regression', \n",
    "    random_state=0)\n",
    "\n",
    "search = nested_cross_val(lgbm, lgbm_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(search.cv_results_)\n",
    "\n",
    "results.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
    "\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results[['params','mean_test_score', 'std_test_score']]\n",
    "\n",
    "results['mean_test_score'].plot(yerr=[results['std_test_score'], results['std_test_score']], subplots=True)\n",
    "\n",
    "plt.ylabel('Mean test score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the predictions\n",
    "\n",
    "X_train_preds = search.predict(X_train)\n",
    "X_test_preds = search.predict(X_test)\n",
    "\n",
    "\n",
    "print('Train rmse: ', mean_squared_error(y_train, X_train_preds, squared=False))\n",
    "print('Test rmse: ', mean_squared_error(y_test, X_test_preds, squared=False))\n",
    "print()\n",
    "print('Train r2: ', r2_score(y_train, X_train_preds))\n",
    "print('Test r2: ', r2_score(y_test, X_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember house price has been logged so we should exponentiate!\n",
    "# np.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_perf = sel.SelectBySingleFeaturePerformance(\n",
    "    estimator=DecisionTreeRegressor(random_state=2, max_depth=2),\n",
    "    scoring='r2',\n",
    "    cv=3,\n",
    "    threshold=0.01,\n",
    "    variables=None,\n",
    ")\n",
    "\n",
    "sel_perf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the attribute feature_performance_ the class stores the performance\n",
    "# of each feature\n",
    "\n",
    "pd.Series(sel_perf.feature_performance_).sort_values().plot.bar(figsize=(15,5))\n",
    "plt.ylabel('r2')\n",
    "\n",
    "# the red line is the threshold we selected.\n",
    "plt.axhline(y = pd.Series(sel_perf.feature_performance_).mean(),\n",
    "            color = 'g', linestyle = '-')\n",
    "\n",
    "# the red line is the threshold we selected.\n",
    "plt.axhline(y = pd.Series(sel_perf.threshold).mean(),\n",
    "            color = 'r', linestyle = '-') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features to drop\n",
    "\n",
    "print('total features: ', X_train.shape[1])\n",
    "print('features to drop: ', len(sel_perf.features_to_drop_))\n",
    "print('remaining features: ', X_train.shape[1] - len(sel_perf.features_to_drop_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# let's try different feature selection method\n",
    "\n",
    "rfm = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=0,\n",
    "    max_depth=2,\n",
    "    )\n",
    "\n",
    "rfe = sel.RecursiveFeatureElimination(\n",
    "    estimator = rfm,\n",
    "    scoring ='neg_root_mean_squared_error',\n",
    "    cv=3,\n",
    "    threshold=0.0001,\n",
    "    variables=None,\n",
    ")\n",
    "\n",
    "rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance of the gbm built using all features\n",
    "\n",
    "rfe.initial_model_performance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the attribute performance_drifts_ the class stores the performance\n",
    "# of each feature the drop in performance when the feature was removed\n",
    "\n",
    "pd.Series(rfe.performance_drifts_).sort_values().plot.bar(figsize=(15,5))\n",
    "plt.ylabel('neg rmse')\n",
    "\n",
    "# the red line is the threshold we selected.\n",
    "plt.axhline(y = pd.Series(rfe.performance_drifts_).mean(),\n",
    "            color = 'g', linestyle = '-')\n",
    "\n",
    "plt.axhline(y = rfe.threshold,\n",
    "            color = 'r', linestyle = '-') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the attribute feature_importances_ the class stores the importance\n",
    "# of each feature, derived from the gbm\n",
    "\n",
    "pd.Series(rfe.feature_importances_).sort_values().plot.bar(figsize=(15,5))\n",
    "plt.ylabel('neg rmse')\n",
    "\n",
    "# the red line is the threshold we selected.\n",
    "plt.axhline(y = pd.Series(rfe.feature_importances_).mean(),\n",
    "            color = 'r', linestyle = '-') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features to drop\n",
    "\n",
    "print('total features: ', X_train.shape[1])\n",
    "print('features to drop: ', len(rfe.features_to_drop_))\n",
    "print('remaining features: ', X_train.shape[1] - len(rfe.features_to_drop_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sel_perf.transform(X_train) \n",
    "X_test = sel_perf.transform(X_test) \n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with cross-validation, searching for best parameters\n",
    "\n",
    "In the rest of the notebook, I will perform a Random Search with cross-validation for the best parameters of a GradientBoostingClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingRegressor(\n",
    "    loss='ls',\n",
    "    n_estimators=100,\n",
    "    criterion='friedman_mse',\n",
    "    min_samples_split=2,\n",
    "    max_depth=3,\n",
    "    random_state=0,\n",
    "    n_iter_no_change=2,\n",
    "    tol=0.0001,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(\n",
    "    loss=['ls', 'huber'],\n",
    "    n_estimators=[10, 20, 50, 100, 200, 500, 1000, 2000],\n",
    "    min_samples_split=[0.1, 0.3, 0.5],\n",
    "    max_depth=[1,2,3,4,None],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = nested_cross_val(gbm, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg = GridSearchCV(gbm, param_grid, scoring='neg_mean_squared_error')\n",
    "\n",
    "# search = reg.fit(X_train, y_train)\n",
    "\n",
    "# search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(search.cv_results_)\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
    "\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results[['params','mean_test_score', 'std_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['params'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['mean_test_score'].plot(yerr=[results['std_test_score'], results['std_test_score']], subplots=True)\n",
    "\n",
    "plt.ylabel('Mean test score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the predictions\n",
    "\n",
    "X_train_preds = search.predict(X_train)\n",
    "X_test_preds = search.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train rmse: ', mean_squared_error(y_train, X_train_preds, squared=False))\n",
    "print('Test rmse: ', mean_squared_error(y_test, X_test_preds, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train r2: ', r2_score(y_train, X_train_preds))\n",
    "print('Test r2: ', r2_score(y_test, X_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023339,
     "end_time": "2021-01-05T18:54:28.519267",
     "exception": false,
     "start_time": "2021-01-05T18:54:28.495928",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light GBM\n",
    "\n",
    "lgbm_param = {\n",
    "    \"num_leaves\": [6, 8, 20, 30],\n",
    "    \"max_depth\": [2, 4, 6, 8, 10],\n",
    "    \"n_estimators\": [50, 100, 200, 500],\n",
    "    'colsample_bytree': [0.3, 1.0],\n",
    "}\n",
    "\n",
    "lgbm = LGBMRegressor(\n",
    "    learning_rate = 0.1, \n",
    "    min_child_weight = 0.4,\n",
    "    objective='regression', \n",
    "    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = nested_cross_val(lgbm, lgbm_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg = GridSearchCV(lgbm, lgbm_param, scoring='neg_mean_squared_error')\n",
    "\n",
    "# search = reg.fit(X_train, y_train)\n",
    "\n",
    "# search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
    "\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results['mean_test_score'].plot(yerr=[results['std_test_score'], results['std_test_score']], subplots=True)\n",
    "\n",
    "plt.ylabel('Mean test score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the predictions\n",
    "X_train_preds = search.predict(X_train)\n",
    "X_test_preds = search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train rmse: ', mean_squared_error(y_train, X_train_preds,squared=False))\n",
    "print('Test rmse: ', mean_squared_error(y_test, X_test_preds,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train r2: ', r2_score(y_train, X_train_preds))\n",
    "print('Test r2: ', r2_score(y_test, X_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
